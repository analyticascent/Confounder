{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Confounder Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How Confounder is Used:** Below is a workflow describing what Confounder needs in order to operate and how it's used in practice. It's strongly recommended that you check out [the README for this project](https://github.com/analyticascent/confounder/blob/master/README.md) first for context.\n",
    "\n",
    "Now for the workflow...\n",
    "\n",
    "* You start with a set of sample documents (news, studies, blog posts, etc) related to a given topic. \n",
    "* Some contain certain pieces of information while others don't mention them at all (binary classification). \n",
    "* Confounder uses this set as training data to \"learn\" to tell the two apart (supervised machine learning).\n",
    "* You repeat this process for any given piece of information you want to check for the presence of.\n",
    "* With the right training data and settings, it can categorize new text passages by what they contain.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**This can be thought of as a much more scalable alternative to constantly using** `Ctrl + F` **and trying to come up with as many synonyms as possible that might indicate whether a document contains specific information or not.**\n",
    "\n",
    "Confounder's focus is particularly on detecting information that pertains to the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Methodological Criteria for Accurate Research](https://raw.githubusercontent.com/analyticascent/confounder/master/images/research_methodology.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side Note:** A *\"confounder\"* in statistics is a [third variable](https://explorable.com/confounding-variables) that can distort what the true relationship is between the original [independent (X) and responding (Y) variables](https://explorable.com/research-variables). Confounders are also sometimes called *extraneous* variables.\n",
    "\n",
    "**Bear in mind that there is no such thing as true \"fact-checking\" software (in part due to [second-order questions](https://www.bloomberg.com/view/articles/2016-12-23/fact-checking-s-infinite-regress-problem)) and thus Confounder is only meant to check if something was even _mentioned_ in a text sample.**\n",
    "\n",
    "**I can't emphasize this enough:** _Checking for omissions is not the same thing as trying to rank how \"true\" or \"false\" a passage of text is._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What This Demo Notebook Illustrates\n",
    "\n",
    "This demo version of [Confounder](https://github.com/analyticascent/confounder) can be repurposed for whatever subject matter or binary text classification task you choose. Doing so only requires that you read in a different CSV file with sample text and labels for what the text contains. \n",
    "\n",
    "Note that each of the criteria you have listed has to be checked independently from others. This will not only allow you to know which specific parts of the criteria are met by a text document, it will also boost the accuracy of the overall results by reducing how much [dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) the classifier must parse through to get the end results.\n",
    "\n",
    "**Demo Subject Matter:** The demo notebook you're reading right now is focused on a specific statistical claim, and a specific trade-off that often gets overlooked when that claim is made:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "![college earnings graph](https://raw.githubusercontent.com/analyticascent/confounder/master/images/college_earnings.gif)\n",
    "\n",
    "* **Claim:** Obtaining a four-year degree will enable you to earn a million dollars more over your lifetime\n",
    "* **Trade-off:** Tuition cost (including interest on student loans) can often cancel out most of those earnings.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "That trade-off is by no means the only issue with the statistic, but for now we will look at training a text classifier to pick up on whether this was mentioned or not. The process would be repeated for these other issues as well, though they are not:\n",
    "\n",
    "* The sunk cost of not working as much (or at all) before obtaining the degree.\n",
    "* Super-earner outliers skewing the average for degree recipients (use the median!).\n",
    "* Different degrees lead to very different lifetime earnings ([new data on this incoming](https://www.insidehighered.com/views/2019/03/26/president-trumps-embrace-program-level-earnings-data-game-changing-opinion)).\n",
    "* Jobs for high-earning majors tend to be in more expensive cities ([San Francisco anyone?](https://www.youtube.com/watch?v=ExgxwKnH8y4)).\n",
    "* Students that complete college may already be prone to succeed (so they [may not be benefit much](https://www.theatlantic.com/magazine/archive/2018/01/whats-college-good-for/546590/)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With all this in mind, let's take a look at the Python code that's involved.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Importing Needed Libraries\n",
    "\n",
    "You will need `pandas` to read in rows and colums (containing the raw article text, and columns for all of the criteria of interest). A **0** in a column indicates a piece of information is absent, while a **1** indicates it was mentioned.\n",
    "\n",
    "`Numpy` adds functionality that you will depend on throughout notebook use. Very specific tools are also imported from `scikit-learn.` Additionally, a few natural language processing tools are imported which may be used to boost model accuracy (with iterative trial and error).\n",
    "\n",
    "Below, we will import those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the \"libraries\" you need to import to run the scripts that follow. It may take several seconds for some to load.\n",
    "\n",
    "import pandas as pd # allows you to read in rows/columns of data\n",
    "import numpy as np # allows you to work with vectors/matrices\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # train_test_split will all you to check accuracy on existing data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # these structure text into word vectors\n",
    "\n",
    "# CountVectorizer turns raw text into word frequency counts, aka \"bag of words.\" Very simple way of structuring text\n",
    "# TfidVectorizer is a little more complex. \"Term Frequency-Inverse Document Frequency\" will find relative importance of terms\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB  # multinomial naive bayes will classify text samples from the vectorizer results\n",
    "from sklearn import metrics  # metrics will be used to evaluate the accuracy of the model when you run train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Reading in the CSV File Containing the Training Corpus\n",
    "\n",
    "As previously mentioned, you will need some labeled text passages in CSV form to train the classifier. **This is how *supervised machine learning* is supposed to work - the columns with \"labels\" of what criteria the sample text failed or met are the categories we are aiming to classify articles under.** \n",
    "\n",
    "The classifier needs to have a sample of various articles to compare future articles against. It is strongly advised that you spend time gathering a varied and large sample of articles that meet or fail as wide a variety of criteria as possible, otherwise Confounder may not be able to predict new samples accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read sampletext.csv into a DataFrame. Any CSV with columns containing raw tweet contents and usernames can often work.\n",
    "# If you're offline, replace the link with the file location for sampletext.csv if you have it stored locally.\n",
    "\n",
    "url = 'https://github.com/analyticascent/confounder/raw/master/data/sampletext.csv'\n",
    "data = pd.read_csv(url, index_col=0, encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IGNORE THIS CELL UNLESS YOU NEED TO SET VARIABLES FOR A DIFFERENT CSV FILE\n",
    "\n",
    "# data = pd.DataFrame({\"variable\": [1,0,1,0,1,1,0,1,1,1,0,0,0,1,0]})\n",
    "\n",
    "# data['variable'] = [1,0,1,0,1,1,0,1,1,1,0,0,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below will be used to verify if the CSV file has loaded properly. You need to have a CSV file with properly labeled columns and rows containing the raw article text (first column), and then a **0** or a **1** within each criteria column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip to Step 3 if you have loaded a CSV file and aren't creating a dataframe from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw_text', 'variable']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the two columns\n",
    "\n",
    "list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`raw_text` is the column that stores the text from articles and studies about college lifetime earnings, while `variable` is used to store `0` or `1` depending on whether tuition cost was mentioned or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMMENTARY \\n\\nBeyond the College Earnings Pre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>  \\n\\ny • By Richard Rothstein • July 21, 200...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(https://dailyreckoning.com/author/ericfry)\\nB...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>  \\n\\nt • By Lawrence Mishel • February 21, 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The college earnings premium is near\\nrecord h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  variable\n",
       "0  COMMENTARY \\n\\nBeyond the College Earnings Pre...         1\n",
       "1    \\n\\ny • By Richard Rothstein • July 21, 200...         0\n",
       "2  (https://dailyreckoning.com/author/ericfry)\\nB...         1\n",
       "3    \\n\\nt • By Lawrence Mishel • February 21, 2...         0\n",
       "4  The college earnings premium is near\\nrecord h...         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first five rows/tweets\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of rows and columns\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we're only working with fifteen articles (the number of rows). Accuracy would be much higher with a greater sample size, but for demo purposes this will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['variable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eight of the articles mention tuition cost, while seven don't. These were labels I added into the CSV file before it was uploaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Define Variables for *train_test_split* Accuracy Experiments\n",
    "\n",
    "Given the contents of the raw article text, did it likely meet or fail a criteria feature? **We will do a train/test split with the training data to measure predictive accuracy for each criteria column.** Define **X** as the raw article text (the manipulated variable), and some **y** variables as the individual criteria columns you are trying to classify future text into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define X and y - the manipulated variable and responding variable\n",
    "\n",
    "X = data.raw_text  # this defines X as the csv column that contains sample article text\n",
    "y = data.variable  # does the text mention tuition cost or not as a trade-off?\n",
    "\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have the columns of the CSV file defined as Python variables. No you just need to *vectorize* the `raw_text` you have stored in the `X` variable, meaning you'll quantify the text in some manner so machine learning can be performed on it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Use *CountVectorizer* to Turn *X_train* and *X_test* into Document-Term Matrices\n",
    "\n",
    "- **What:** Turn the training and testing portions of your framework samples into *document-term matrices*\n",
    "- **Why:** Gives structure to previously unstructured text; you now have word frequency counts\n",
    "- **Notes:** Easier with English text, not easy with langauges where beginning/end of words or sentences is ambigous\n",
    "\n",
    "We are now going to create what are called *document-term matrices* of the sample articles. **Think of these as *rows and columns* which store numbers representing *how often* certain terms appear in different text samples.** See the image below to better understand what that looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "![Document-Term Matrix](https://raw.githubusercontent.com/analyticascent/confounder/master/images/text_vectorization.png)\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the frequency of certain words (and word sequences) will be different in text samples that mention tuition cost versus those that don't mention it at all. It's this difference that an algorithm \"learns\" to use to tell the difference between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "\n",
    "vect = CountVectorizer()  # because vect is way easier to type than CountVectorizer\n",
    "X_train_dtm = vect.fit_transform(X_train)  # stores a vectorized X_train sample into X_train_dtm\n",
    "X_test_dtm = vect.transform(X_test)  # stores a vectorized X_train sample into X_train_dtm\n",
    "\n",
    "# now we have quantitative info about articles that classifier can work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just to clarify what's going on in the adjacent cells:** All the **rows** are of course the *individual tweets* that are stored in the CSV file. But the astronomical crapload of **columns** is literally *each unique term* that appears. Those are going to be the \"features\" used to \"fingerprint\" one user from another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 6343)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows are documents, columns are terms (aka \"tokens\" or \"features\")\n",
    "\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zimmermann', 'zt', 'zukunft', 'zur', 'zxx', 'ˆα', 'ˆπ', 'ˆρ', 'ˆτ', 'ˆτb', 'ˆτsim', 'β0', 'β1i', 'βf', 'βijt', 'γ0', 'γ1f', 'γ2f', 'γf', 'δ0', 'δ1f', 'δ2f', 'δf', 'ηi', 'θ0', 'θ1f', 'θ2f', 'θf', 'μi', 'πi', 'τb', 'ﬁeld', 'ﬁelds', 'ﬁg', 'ﬁgure', 'ﬁgures', 'ﬁle', 'ﬁnal', 'ﬁnance', 'ﬁnancial', 'ﬁnancing', 'ﬁnd', 'ﬁnds', 'ﬁrst', 'ﬁt', 'ﬁve', 'ﬁxed', 'ﬂagship', 'ﬂat', 'ﬂexible']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parameter **lowercase:** boolean, True by default\n",
    "    - If True, Convert all characters to lowercase before tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 7371)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will not convert to lowercase this time, but if we did it would reduce the number of quantified features\n",
    "\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yu', 'zXX', 'zero', 'zi', 'zur', 'ˆα', 'ˆπ', 'ˆρ', 'ˆτ', 'ˆτb', 'ˆτsim', 'β0', 'β1I', 'βf', 'βijt', 'γ0', 'γ1f', 'γ2f', 'γf', 'δ0', 'δ1f', 'δ2f', 'δf', 'ηi', 'θ0', 'θ1f', 'θ2f', 'θf', 'μi', 'πI', 'τb', 'ﬁeld', 'ﬁelds', 'ﬁg', 'ﬁgure', 'ﬁgures', 'ﬁle', 'ﬁnal', 'ﬁnance', 'ﬁnancial', 'ﬁnancing', 'ﬁnd', 'ﬁnds', 'ﬁrst', 'ﬁt', 'ﬁve', 'ﬁxed', 'ﬂagship', 'ﬂat', 'ﬂexible']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the cell will allow you to augment how *CountVectorizer* works by including a range of **n-grams.** These are *word sequences,* so a 2-gram for instance will be a *pair* of words. The result from including *that* range is that the resulting *document-term matrices* will contain frequency counts of how often pairs of words appear, as well as single terms if you specify so as a parameter. \n",
    "\n",
    "- Parameter **ngram_range:** tuple (min_n, max_n)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 40655)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1, 2))  # sets the vectorizer to look at single as well as pairs of words\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ﬁle', 'ﬁle of', 'ﬁnal', 'ﬁnal exit', 'ﬁnal graduation', 'ﬁnance', 'ﬁnance and', 'ﬁnancial', 'ﬁnancial payoff', 'ﬁnancing', 'ﬁnancing of', 'ﬁnd', 'ﬁnd an', 'ﬁnd are', 'ﬁnd evidence', 'ﬁnd of', 'ﬁnd positive', 'ﬁnd that', 'ﬁnds', 'ﬁnds that', 'ﬁrst', 'ﬁrst 15', 'ﬁrst estimate', 'ﬁrst outcome', 'ﬁrst paper', 'ﬁrst row', 'ﬁrst sample', 'ﬁrst stage', 'ﬁrst years', 'ﬁt', 'ﬁt to', 'ﬁve', 'ﬁve in', 'ﬁve types', 'ﬁve universities', 'ﬁxed', 'ﬁxed at', 'ﬁxed eﬀects', 'ﬁxed number', 'ﬂagship', 'ﬂagship institution', 'ﬂagship state', 'ﬂagship university', 'ﬂat', 'ﬂat along', 'ﬂexible', 'ﬂexible below', 'ﬂexible linear', 'ﬂexible polynomial', 'ﬂexible polynomials']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Test Predictive Accuracy for First Criteria Feature\n",
    "\n",
    "How accurate can we predict whether a text sample met or failed a criteria compared to the way you originally labeled it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# use default options for CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "\n",
    "\n",
    "# create document-term matrices using CountVectorizer, and store them as variables\n",
    "\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "\n",
    "# use Naive Bayes to predict the first feature of the list criteria\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "\n",
    "# calculate accuracy\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# use default options for CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "\n",
    "\n",
    "# create document-term matrices using CountVectorizer, and store them as variables\n",
    "\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "\n",
    "# use Naive Bayes to predict the first feature of the list criteria\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "\n",
    "# calculate accuracy\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will eliminate the need for typing in the same code over and over again, as well as produce an output that includes all the information we need to know about how the number of unique features is affecting the classifier accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a vectorizer and calculates the accuracy\n",
    "\n",
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print('Features: ', X_train_dtm.shape[1])  # this output will be unique words or n-grams\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  6343\n",
      "Accuracy:  75.0 %\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Test Stopword Removal and N-Grams to Potentially Boost Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  40655\n",
      "Accuracy:  75.0 %\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Remove common words that will likely appear in any text\n",
    "- **Why:** They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "    - If 'english', a built-in stop word list for English is used.\n",
    "    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    - If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  6086\n",
      "Accuracy:  75.0 %\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words\n",
    "\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'nowhere', 'several', 'even', 'thereby', 'a', 'towards', 'twenty', 'afterwards', 'would', 'amoungst', 'together', 'yours', 'seem', 'see', 'by', 'system', 'name', 'latter', 'while', 'except', 'ten', 'toward', 'ltd', 'up', 'than', 'others', 'too', 'beside', 'therefore', 'due', 'and', 'becoming', 'your', 'should', 'over', 'beyond', 'whence', 'along', 'hereupon', 'often', 'somehow', 'has', 'ever', 'about', 'was', 'out', 'we', 'my', 'never', 'be', 'seemed', 'get', 'she', 'something', 'under', 'were', 'almost', 'third', 'hundred', 'describe', 'hers', 'put', 'else', 'only', 'though', 'fifty', 'well', 'everything', 'amount', 'any', 'already', 'on', 'against', 'one', 'when', 'may', 'none', 'of', 'themselves', 'myself', 'anyone', 'empty', 'became', 'its', 'thru', 'sometime', 'thus', 'either', 'at', 'you', 'whole', 'mostly', 'itself', 'however', 'all', 'cannot', 'thin', 'us', 'three', 'least', 'indeed', 'whose', 'thereupon', 'could', 'thereafter', 'our', 'otherwise', 'eleven', 'herself', 'becomes', 'me', 'some', 'couldnt', 'an', 'but', 'everyone', 'move', 'whoever', 'himself', 'everywhere', 'nothing', 'nobody', 'also', 'less', 'perhaps', 'every', 'somewhere', 'please', 'into', 'few', 'meanwhile', 'someone', 'sometimes', 'yourselves', 'whenever', 'former', 'do', 'her', 'become', 'i', 'keep', 'part', 'after', 'if', 'those', 'whether', 'further', 'because', 'around', 'fire', 'seeming', 'within', 'wherein', 'cant', 'this', 'off', 'serious', 'the', 'take', 'seems', 'until', 'more', 'thence', 'anywhere', 'what', 'another', 'ie', 'sincere', 'with', 'last', 'next', 'own', 'wherever', 'once', 'back', 'so', 'done', 'ourselves', 'again', 'he', 'six', 'then', 'therein', 'go', 'such', 'un', 'alone', 'enough', 'between', 'throughout', 'who', 'behind', 'five', 'bill', 'have', 'other', 'etc', 'latterly', 'whereupon', 'formerly', 'side', 'below', 'not', 'moreover', 'before', 'as', 'fill', 'give', 'without', 'first', 'same', 'rather', 'whereby', 'his', 'for', 'from', 'co', 'nor', 'con', 'very', 'which', 'much', 'since', 'nine', 'in', 'can', 'hereafter', 'mine', 'where', 'hereby', 'neither', 'why', 'besides', 'via', 'being', 'mill', 'anyway', 'nevertheless', 'twelve', 'amongst', 'two', 'might', 'anything', 'them', 'beforehand', 'eight', 'full', 'many', 'hasnt', 'they', 'eg', 'thick', 'show', 'to', 'both', 'made', 'through', 'whereafter', 'cry', 'onto', 'must', 'find', 'call', 'there', 'noone', 'always', 'top', 'are', 'per', 'front', 'had', 'no', 'whom', 'will', 'hence', 'during', 'detail', 'that', 'whereas', 'inc', 'upon', 'each', 'found', 'it', 'still', 'ours', 'been', 'most', 'how', 'fifteen', 'sixty', 'yourself', 'bottom', 'or', 'above', 'anyhow', 'four', 'across', 'de', 'down', 'now', 'their', 'herein', 'whatever', 'these', 'yet', 'am', 'here', 'although', 'among', 'forty', 'him', 'interest', 'is', 'namely', 're', 'elsewhere', 'whither'})\n"
     ]
    }
   ],
   "source": [
    "# set of stop words\n",
    "\n",
    "print(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Other CountVectorizer Options to Raise Predictive Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_features:** int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  25\n",
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=25) # I need a minimum of only 25 unique vocabulary terms\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  16\n",
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words and only keep 16 features\n",
    "\n",
    "vect = CountVectorizer(stop_words='english', max_features=16) # Removing stop words brings prior requirement down to 16\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['changes', 'college', 'degree', 'demand', 'education', 'graduates', 'high', 'labor', 'relative', 'returns', 'school', 'university', 'wage', 'wages', 'workers', 'year']\n"
     ]
    }
   ],
   "source": [
    "# all 100 features\n",
    "\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  150\n",
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and limit the number of features\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=150)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '10', '16', '1963', '1980s', '1987', '2000', '2009', 'admission', 'all', 'also', 'an', 'analysis', 'and', 'and the', 'are', 'as', 'at', 'at the', 'average', 'be', 'been', 'between', 'but', 'by', 'can', 'changes', 'changes in', 'china', 'college', 'college graduates', 'data', 'degree', 'demand', 'each', 'earnings', 'economic', 'economics', 'education', 'elite', 'elite university', 'employment', 'estimates', 'experience', 'figure', 'for', 'for the', 'from', 'from the', 'graduates', 'group', 'growth', 'has', 'have', 'high', 'high school', 'higher', 'if', 'in', 'in relative', 'in the', 'income', 'industry', 'inequality', 'is', 'is the', 'it', 'it is', 'job', 'jobs', 'journal', 'journal of', 'labor', 'less', 'market', 'more', 'most', 'not', 'of', 'of college', 'of the', 'ols', 'on', 'on the', 'one', 'only', 'or', 'other', 'our', 'over', 'over the', 'percent', 'period', 'premium', 'relative', 'relative wages', 'returns', 'returns to', 'rural', 'sample', 'school', 'schooling', 'score', 'sector', 'share', 'shifts', 'since', 'some', 'students', 'supply', 'table', 'than', 'that', 'that the', 'the', 'the college', 'the elite', 'the relative', 'their', 'there', 'these', 'they', 'this', 'those', 'those with', 'threshold', 'time', 'to', 'to education', 'to the', 'total', 'unemployment', 'university', 'urban', 'use', 'wage', 'wages', 'was', 'we', 'were', 'which', 'who', 'will', 'with', 'within', 'work', 'workers', 'would', 'year', 'years']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  40655\n",
      "Accuracy:  75.0 %\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  100\n",
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and limit the number of features\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "    - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  29877\n",
      "Accuracy:  75.0 %\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and only include terms that appear at least 2 times\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  150\n",
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words and only keep 100 features\n",
    "\n",
    "vect = CountVectorizer(stop_words='english', max_features=150)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** _The perfect accuracy here is likely the result of overfitting on the training data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '10', '11', '12', '15', '16', '17', '18', '19', '1963', '1975', '1979', '1980s', '1987', '1991', '1995', '20', '2000', '2001', '2004', '2005', '2007', '2009', '2010', '25', '30', 'admission', 'advanced', 'al', 'analysis', 'available', 'average', 'bachelor', 'bureau', 'business', 'change', 'changes', 'china', 'college', 'column', 'data', 'degree', 'degrees', 'demand', 'differences', 'differentials', 'discontinuity', 'distribution', 'earnings', 'economic', 'economics', 'educated', 'education', 'elite', 'employment', 'epi', 'estimate', 'estimates', 'et', 'evidence', 'experience', 'factor', 'female', 'figure', 'given', 'graduates', 'group', 'groups', 'growth', 'high', 'higher', 'important', 'income', 'increase', 'increased', 'index', 'industry', 'inequality', 'job', 'jobs', 'journal', 'just', 'labor', 'level', 'li', 'log', 'major', 'male', 'market', 'measure', 'median', 'new', 'number', 'ols', 'openings', 'percent', 'percentage', 'period', 'premium', 'private', 'production', 'rate', 'ratio', 'recent', 'recession', 'reform', 'regression', 'relative', 'research', 'return', 'returns', 'rural', 'sample', 'school', 'schooling', 'score', 'sector', 'share', 'shift', 'shifts', 'shows', 'skills', 'standard', 'students', 'studies', 'study', 'supply', 'table', 'term', 'threshold', 'time', 'total', 'trade', 'trend', 'unemployed', 'unemployment', 'university', 'urban', 'use', 'using', 'value', 'variable', 'wage', 'wages', 'women', 'work', 'workers', 'year', 'years', 'young']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names()) # notice that only 150 features are listed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Using GridSearchCV for Hyperparameter Opitimization\n",
    "\n",
    "Although it wasn't covered comprehensively in the part-time course I took, it turns out there is a rather simple way to do multiple train/test splits on the same data with cross-validation (to avoid overfitting), as well as try out a variety of parameter settings to see which ones output the most predictive results. \n",
    "\n",
    "**Enter GridSearchCV.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[('vectorize',CountVectorizer()),\\\n",
    "       ('clf',MultinomialNB())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=\\\n",
    "train_test_split(data['raw_text'], data['variable'], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorize', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.500000\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy = {:3f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vectorize', 'clf'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(vectorize__binary=[True,False],\\\n",
    "                  vectorize__stop_words=[None,'english'],\\\n",
    "                  #vectorize__min_df=[1,3,5,7,9,12],\\\n",
    "                  vectorize__lowercase=[True,False],\\\n",
    "                  vectorize__ngram_range=[(1,1),(1,2)],\\\n",
    "                  vectorize__max_features=[95,100,105] # did a lot of iteration to whittle it down to those\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe, param_grid=param_grid,\\\n",
    "                           scoring=make_scorer(accuracy_score),n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/analyticascent/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 672 ms, sys: 68 ms, total: 740 ms\n",
      "Wall time: 10.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/analyticascent/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%time res=grid_search.fit(data['raw_text'], data['variable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorize__binary': True,\n",
       " 'vectorize__lowercase': True,\n",
       " 'vectorize__max_features': 95,\n",
       " 'vectorize__ngram_range': (1, 1),\n",
       " 'vectorize__stop_words': 'english'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" + str(res.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keep in mind that the above result is the best outcome we could get with the specified random state.** Earlier accuracy results in this notebook without `GridSearchCV` that led to 100% accuracy were probably the result of overfitting to the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Concluding Tips and Tricks\n",
    "\n",
    "- It will always be easier to determine if an article *omits* something than screen how \"true\" a statement is\n",
    "- This is all only as good as the criteria you set out to check articles and studies for\n",
    "- That criteria is also a large influence on how good your training sample is\n",
    "- Your workflow should check each of the criteria independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
